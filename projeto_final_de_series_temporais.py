# -*- coding: utf-8 -*-
"""Projeto Final de Series Temporais.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O6NQku9OhSkdwUWCprRcWXgUdzOW6V8p

## PÓS-GRADUAÇÃO EM CIENCIAS DE DADOS

## SERIES TEMPORAIS - PROJETO FINAL

**Professor: Paulo Salgado**

Grupo:

Karoline Juliana Costa da Silva

Michele Vanessa Sercundes Nunes

Maria Paula Souza de Azevedo

A Base de daos escolhida é venda_varejo_pe.csv disponibilidada no github - https://github.com/Eraylson/series_temporais


O propósito deste trabalho é prever o comportamento de vendas no proximos 12 meses, de acordo com o comportamento das vendas anteriores.

**Bibliotecas**
"""

!pip install pmdarima

# lib to handle with time series
import numpy as np
import pandas as pd
# lib to plot the data

# lib to use the decompositio in the time series
from statsmodels.tsa.seasonal import seasonal_decompose
# lib to use the acf and pcf correlations
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import statsmodels.api as sm


import matplotlib.pyplot as plt
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 15, 6


from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_absolute_percentage_error as mape



import pmdarima as pm
from pmdarima.arima import auto_arima, arima
#from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.arima.model import ARIMA
# from pmdarima.utils import tsdisplay
from pmdarima.preprocessing import BoxCoxEndogTransformer
from pmdarima.model_selection import train_test_split
from pmdarima.pipeline import Pipeline


from statsmodels.stats.diagnostic import acorr_ljungbox


from matplotlib.pylab import rcParams
rcParams['figure.figsize'] =10, 6


import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

#Função para mostrar a autocorrelação e autocorrelação parcial lado a lado
def acf_pacf(x, qtd_lag):
    fig = plt.figure(figsize=(16,10))
    ax1 = fig.add_subplot(221)
    fig = sm.graphics.tsa.plot_acf(x, lags=qtd_lag, ax=ax1)
    ax2 = fig.add_subplot(222)
    fig = sm.graphics.tsa.plot_pacf(x, lags=qtd_lag, ax=ax2)
    plt.show()

# Leitura da série temporal diretamento do GitHub
# URL do arquivo CSV no GitHub
url = 'https://raw.githubusercontent.com/Eraylson/series_temporais/master/vendas_varejo_pe.csv'

# Lendo os dados do CSV
dados = pd.read_csv(url, sep = ';', parse_dates=['data'], index_col='data', decimal=',')
dados.head()

dados.index = pd.date_range('1/1/2000', periods=189, freq='M', normalize =True)

dados

serie = dados['Venda']

serie.isna().sum()

serie.info()

plt.plot(serie)
plt.show()

from pmdarima import arima
from pmdarima import datasets
from pmdarima import utils

figure_kwargs = {'figsize': (15, 6)}  #
# Decomposição dos dados das vendas em tendencia, sazonalidade e ruido
decomposed = arima.decompose(serie.values.reshape(-1),
                             'additive', m=12) # foi escolhido m=12 por serem dados mensais, e assim vermos o comportamento em um ano

axes = utils.decomposed_plot(decomposed, figure_kwargs=figure_kwargs,
                             show=False)
axes[0].set_title("Vendas a varejo - Decomposição")

qtd_lag = 24
acf_pacf(serie, qtd_lag)

"""Como para a Auto correlação não tem corte brusco, mostra que é uma série mais complicada de prever a tendencia e a sazonalidade, para o modelo arima deve ser considerado o p=0.

Já o caso da Autocorrelação parcial, existe um corte brusco após a segunda linha, o que indica o q=2 para o modelo arima.

**teste de estacionariedade da série**
"""

# Teste de estacionariedade - Dickey-Fuller (ADF)
from statsmodels.tsa.stattools import adfuller
result = adfuller(serie)

print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

if result[1] <= 0.05:
    print('A série é estacionária.')
else:
    print('A série não é estacionária.')

serie_1 = serie - serie.shift(1)
serie_1

serie_1_ordem = serie_1.dropna()
serie_1_ordem

# Teste de estacionariedade aumentado de Dickey-Fuller (ADF) para a série temporal diferenciada de 1ª ordem
result = adfuller(serie_1_ordem)

print('Estatística ADF:', result[0])
print('p-value:', result[1])
print('Valores Críticos:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

if result[1] <= 0.05:
    print('A série é estacionária.')
else:
    print('A série não é estacionária.')

serie_2 = serie_1_ordem - serie_1_ordem.shift(1)
serie_2

serie_2_ordem = serie_2.dropna()
serie_2_ordem

# Teste de estacionariedade aumentado de Dickey-Fuller (ADF) para a série temporal diferenciada de 1ª ordem
result = adfuller(serie_2_ordem)

print('Estatística ADF:', result[0])
print('p-value:', result[1])
print('Valores Críticos:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

if result[1] <= 0.05:
    print('A série é estacionária.')
else:
    print('A série não é estacionária.')

"""Como a série inicial não é estácionaria, isso significa que precisa fazer diferenciação até que ela fique estácionaria para poder indicar no modelo arima, ou seja o d=2. Assim o ruido estará livre de tendência e Sazonalidade.

**Normalização da Série**
"""

# Serie normalizada
scaler = MinMaxScaler()

serie_normalizada = scaler.fit_transform(serie_2_ordem.values.reshape(-1, 1)).flatten()

plt.plot(serie_normalizada)

"""**Divisão de Bases**"""

# Divide os dados em conjuntos de treinamento, validação e teste
train_size = int(len(serie_normalizada) * 0.5)
validation_size = int(len(serie_normalizada) * 0.25)
test_size = len(serie_normalizada) - train_size - validation_size

train, validation, test = serie_normalizada[:train_size], serie_normalizada[train_size:train_size+validation_size], serie_normalizada[train_size+validation_size:]
print("Tamanho da base de treinamento", len(train))
print("Tamanho da base de validação", len(validation))
print("Tamanho da base de teste", len(test))

"""**Definição do Modelo Arima**"""

# Ajustando o modelo ARIMA aos dados de treinamento

modelo_ARIMA = pm.arima.ARIMA(order=(0,2,1))
modelo_ARIMA.fit(train)

# Previsões no conjunto de treino
predicoes_train_ARIMA = modelo_ARIMA.predict_in_sample()

rcParams['figure.figsize'] = 15, 6
plt.plot(predicoes_train_ARIMA[12:], label = 'Predições-ARIMA (treino)')
plt.plot(train[12:], label = 'Valores reais (treino)')
plt.legend()

predicoes_train_ARIMA

# Criar um índice de datas com base no comprimento da série
date_index = pd.date_range(start='1/1/2000', periods=len(train), freq='M')

train_series = pd.Series(train, index=date_index)

# Calcular os resíduos
residuos = train - predicoes_train_ARIMA

residuos

# Gráfico dos resíduos ao longo do tempo
plt.figure(figsize=(12, 6))
plt.plot(date_index, residuos)
plt.title('Resíduos do Modelo ARIMA (Treinamento)')
plt.xlabel('Data')
plt.ylabel('Resíduos')
plt.show()

# Histograma dos resíduos
plt.figure(figsize=(8, 6))
plt.hist(residuos, bins=20)
plt.title('Histograma dos Resíduos')
plt.xlabel('Resíduos')
plt.ylabel('Frequência')
plt.show()

# Gráfico Q-Q dos resíduos
import scipy.stats as stats
plt.figure(figsize=(8, 6))
stats.probplot(residuos, dist="norm", plot=plt)
plt.title('Gráfico Q-Q dos Resíduos')
plt.show()

# Função de Autocorrelação dos Resíduos
plot_acf(residuos, lags=20)
plt.title('Função de Autocorrelação dos Resíduos')
plt.show()

from statsmodels.stats.diagnostic import acorr_ljungbox

# Teste de Ljung-Box para independência serial dos resíduos
lb_test = acorr_ljungbox(residuos, lags=12)
print("Valores-p do Teste Ljung-Box:", lb_test['lb_pvalue'])

"""Para P-valores tão pequenos, é possivel prerceber que o modelo precisa de ajustes, pois ainda existe correlação nos lags."""

residuos_pd = pd.Series(residuos, index=date_index)

rolling = residuos_pd.interpolate(method = 'polynomial', order=10)

plt.plot(residuos_pd,color='blue', label='Série Real')
rolling_mean = rolling.mean()
plt.plot(rolling,color='red', label='Polinomial')
plt.legend(loc='best')
plt.show()

dados_sazonalidade_ruido = residuos_pd - rolling_mean  # Série - componente de tendência

plt.plot(dados_sazonalidade_ruido, label='Sem Tendência')
plt.legend(loc='best')
plt.show()

residuos_pd.shift()

residuos_diff = residuos_pd - residuos_pd.shift()  #Subtrai a série por ela mesma com um retardo -1

plt.plot(residuos_diff, label='Diff 1º Ordem')
plt.legend(loc='best')
plt.show()

residuos_diff_2 = residuos_diff - residuos_diff.shift()

plt.plot(residuos_diff_2, label='Diff 2º Ordem')
plt.legend(loc='best')
plt.show()

#Aplica média móveis na série sem a tendência
dados_sazonalidade_ruido

rolling = dados_sazonalidade_ruido.rolling(window=12)
rolling_mean = rolling.mean()
# plot original and transformed dataset
plt.plot(dados_sazonalidade_ruido, label='Série sem tendência')
plt.plot(rolling_mean,color='red', label='Média Móveis')
plt.legend(loc='best')
plt.show()

dados_ruido = dados_sazonalidade_ruido - rolling_mean #Elimina o componente de sazonalidade

plt.plot(dados_ruido, label='Ruído')
plt.legend(loc='best')
plt.show()



"""**Avaliando o Resíduo**"""

import statsmodels.tsa.api as smt

dados_ruido.dropna(inplace=True) # retira os NaN
smt.graphics.plot_pacf(dados_ruido, lags=20, alpha=0.5)
plt.show()



"""**Previsão do conjunto de teste**"""

# Previsões no conjunto de teste
# Cálculo da previsão de uma instância
def forecast_one_step():
    fc = modelo_ARIMA.predict(n_periods=1, return_conf_int=False)
    return fc.tolist()[0]
    # np.asarray(conf_int).tolist()[0])

# Lista para armazenamento dos valores das previsões calculadas para cada instância
previsoes_test_ARIMA = []
# confidence_intervals = []

for new_ob in test:
    fc = forecast_one_step()
    previsoes_test_ARIMA.append(fc)
    # confidence_intervals.append(conf)

    # Updates the existing model with a small number of MLE steps
    modelo_ARIMA.update(new_ob)

rcParams['figure.figsize'] = 15, 6
plt.plot(test, label = 'Valores reais (teste)')
plt.plot(previsoes_test_ARIMA, label = 'Predições-ARIMA (teste)')
plt.legend()

m=1

print("freq", m)
print("Test MSE: %.10f" % mean_squared_error(test, previsoes_test_ARIMA))
print("Test MAE: %.10f" % mae(test, previsoes_test_ARIMA))



"""**MODELO MACHINE LEARNE - RANDOM FOREST**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error as MSE

import statsmodels.api as sm

from matplotlib.pylab import rcParams
rcParams['figure.figsize'] =15, 6

# URL do arquivo CSV no GitHub
url = 'https://raw.githubusercontent.com/Eraylson/series_temporais/master/vendas_varejo_pe.csv'

# Lendo os dados do CSV
dados = pd.read_csv(url, sep = ';', parse_dates=['data'], index_col='data', decimal=',')
dados.head()

dados.index = pd.date_range('1/1/2000', periods=189, freq='M', normalize =True)

plt.plot(dados)
plt.show()

def normalizar_serie(serie):
    minimo = min(serie)
    maximo = max(serie)
    y = (serie - minimo) / (maximo - minimo)
    return y

def desnormalizar(serie_atual, serie_real):
    minimo = min(serie_real)
    maximo = max(serie_real)

    serie = (serie_atual * (maximo - minimo)) + minimo

    return pd.DataFrame(serie)

serie = dados.values
serie_normalizada = normalizar_serie(serie)

plt.plot(serie_normalizada)
plt.show()

serie_normalizada

def gerar_janelas(tam_janela, serie):
    # serie: vetor do tipo numpy ou lista
    tam_serie = len(serie)
    tam_janela = tam_janela +1 # Adicionado mais um ponto para retornar o target na janela

    janela = list(serie[0:0+tam_janela]) #primeira janela p criar o objeto np
    janelas_np = np.array(np.transpose(janela))

    for i in range(1, tam_serie-tam_janela):
        janela = list(serie[i:i+tam_janela])
        j_np = np.array(np.transpose(janela))

        janelas_np = np.vstack((janelas_np, j_np))


    return janelas_np

qtd_lags = 12
janelas = gerar_janelas(12, serie_normalizada)

janelas

janelas.shape

def diferenciar_serie(serie):
    #serie: obj pandas
    return serie.diff().dropna()

def incrementar_serie(serie_real, serie_diff):
    return serie_real[0:-1] + serie_diff

serie_diff = diferenciar_serie(dados)

plt.plot(serie_diff)
plt.show()

serie_rt = incrementar_serie(dados.values, serie_diff.values)

plt.plot(serie_rt)
plt.show()

def select_lag_acf(serie, max_lag):
    from statsmodels.tsa.stattools import acf
    x = serie[0: max_lag+1]

    acf_x, confint = acf(serie, nlags=max_lag, alpha=.05, fft=False)

    limiar_superior = confint[:, 1] - acf_x
    limiar_inferior = confint[:, 0] - acf_x

    lags_selecionados = []

    for i in range(1, max_lag+1):


        if acf_x[i] >= limiar_superior[i] or acf_x[i] <= limiar_inferior[i]:
            lags_selecionados.append(i-1)  #-1 por conta que o lag 1 em python é o 0

    #caso nenhum lag seja selecionado, essa atividade de seleção para o gridsearch encontrar a melhor combinação de lags
    if len(lags_selecionados)==0:


        print('NENHUM LAG POR ACF')
        lags_selecionados = [i for i in range(max_lag)]

    print('LAGS', lags_selecionados)

    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    #inverte o valor dos lags para usar na lista de dados se os dados forem de ordem [t t+1 t+2 t+3]
    lags_selecionados = [max_lag - (i+1) for i in lags_selecionados]
    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    return lags_selecionados

sm.graphics.tsa.plot_acf(serie_normalizada, lags=24)

lag_sel = select_lag_acf(serie_normalizada, 24)

lag_sel

def split_serie_with_lags(serie, perc_train, perc_val = 0):

    #faz corte na serie com as janelas já formadas

    x_date = serie[:, 0:-1]
    y_date = serie[:, -1]

    train_size = np.fix(len(serie) *perc_train)
    train_size = train_size.astype(int)

    if perc_val > 0:
        val_size = np.fix(len(serie) *perc_val).astype(int)


        x_train = x_date[0:train_size,:]
        y_train = y_date[0:train_size]
        print("Particao de Treinamento:", 0, train_size  )

        x_val = x_date[train_size:train_size+val_size,:]
        y_val = y_date[train_size:train_size+val_size]

        print("Particao de Validacao:",train_size, train_size+val_size)

        x_test = x_date[(train_size+val_size):-1,:]
        y_test = y_date[(train_size+val_size):-1]

        print("Particao de Teste:", train_size+val_size, len(y_date))

        return x_train, y_train, x_test, y_test, x_val, y_val

    else:

        x_train = x_date[0:train_size,:]
        y_train = y_date[0:train_size]

        x_test = x_date[train_size:-1,:]
        y_test = y_date[train_size:-1]

        return x_train, y_train, x_test, y_test

tam_janela = 12
serie_janelas = gerar_janelas(tam_janela, serie_normalizada)

serie_janelas

x_train, y_train, x_test, y_test, x_val, y_val = split_serie_with_lags(serie_janelas, 0.50, perc_val = 0.25)

from sklearn.ensemble import RandomForestRegressor
def treinar_rf(x_train, y_train, x_val, y_val, num_exec):
    n_estimators =  [1,10, 20, 50, 100]
    criterion = ['squared_error', 'absolute_error']
    max_features = ['auto', 'sqrt', 'log2']
    qtd_lags_sel = len(x_train[0])
    best_result = np.Inf
    for i in range(0,len(n_estimators)):
        for j in range(0,len(criterion)):
            for z in range(0, len(max_features)):
                for qtd_lag in range(1, len(x_train[0]+1)): #variar a qtd de pontos utilizados na janela
                    print('QTD de Lags:', qtd_lag, 'Qtd de Estimadores' ,n_estimators[i], 'Critério de Separação', criterion[j], 'Max Features', max_features[z])
                    for k in range(0,num_exec):
                        rf = RandomForestRegressor(n_estimators=n_estimators[i], criterion=criterion[j], max_features=max_features[k])
                        rf.fit(x_train[:,-qtd_lag:], y_train)
                        predict_validation = rf.predict(x_val[:,-qtd_lag:])
                        mse = MSE(y_val, predict_validation)
                        if mse < best_result:
                            best_result = mse
                            print('Melhor MSE:', best_result)
                            select_model = rf
                            qtd_lags_sel = qtd_lag

    print('QTD de Lags:', qtd_lag, 'Qtd de Estimadores' ,n_estimators[i], 'Critério de Separação', criterion[j], 'Max Features', max_features[k])
    return select_model, qtd_lags_sel

modelo_rf, lag_sel_rf = treinar_rf(x_train, y_train, x_val, y_val, 2)

modelo_rf

modelo_rf.get_params()

lag_sel_rf

predict_train_rf = modelo_rf.predict(x_train[:, -lag_sel_rf:])
predict_val_rf = modelo_rf.predict(x_val[:, -lag_sel_rf:])
predict_test_rf = modelo_rf.predict(x_test[:, -lag_sel_rf:])

previsoes_train_rf = np.hstack(( predict_train_rf, predict_val_rf))
target_train_rf = np.hstack((y_train, y_val))

plt.plot(previsoes_train_rf, label = 'Prev Tr + val')
plt.plot(target_train_rf, label='Target')
plt.legend(loc='best')
plt.show()

plt.plot(predict_test_rf, label = 'Prev Test')
plt.plot(y_test, label='Target')
plt.legend(loc='best')
plt.show()

MSE(y_test, predict_test_rf)

"""**TREINANDO O MLP**"""

def treinar_mlp(x_train, y_train, x_val, y_val, num_exec):

    neuronios =  [1, 10, 20]    #[1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 150, 170, 200]
    func_activation =  ['tanh', 'relu']   #['identity', 'tanh', 'relu']
    alg_treinamento = ['lbfgs']#, 'sgd', 'adam']
    max_iteracoes = [1000] #[100, 1000, 10000]
    learning_rate = ['constant']  #['constant', 'invscaling', 'adaptive']
    qtd_lags_sel = len(x_train[0])
    best_result = np.Inf
    for i in range(0,len(neuronios)):
        for j in range(0,len(func_activation)):
            for l in range(0,len(alg_treinamento)):
                for m in range(0,len(max_iteracoes)):
                    for n in range(0,len(learning_rate)):
                        for qtd_lag in range(1, len(x_train[0]+1)): #variar a qtd de pontos utilizados na janela

                            print('QTD de Lags:', qtd_lag, 'Qtd de Neuronios' ,neuronios[i], 'Func. Act', func_activation[j])


                            for e in range(0,num_exec):
                                mlp = MLPRegressor(hidden_layer_sizes=neuronios[i], activation=func_activation[j], solver=alg_treinamento[l], max_iter = max_iteracoes[m], learning_rate= learning_rate[n])


                                mlp.fit(x_train[:,-qtd_lag:], y_train)
                                predict_validation = mlp.predict(x_val[:,-qtd_lag:])
                                mse = MSE(y_val, predict_validation)

                                if mse < best_result:
                                    best_result = mse
                                    print('Melhor MSE:', best_result)
                                    select_model = mlp
                                    qtd_lags_sel = qtd_lag

    print('QTD de Lags:', qtd_lags_sel, 'Func. Act', func_activation[j])

    return select_model, qtd_lags_sel

modelo_mlp, lag_sel = treinar_mlp(x_train, y_train, x_val, y_val, 2)

modelo_mlp

"""**TESTANDO O MLP**"""

predict_train = modelo_mlp.predict(x_train[:, -lag_sel:])
predict_val = modelo_mlp.predict(x_val[:, -lag_sel:])
predict_test_mlp = modelo_mlp.predict(x_test[:, -lag_sel:])

plt.plot(previsoes_train, label = 'Prev Tr + val')
plt.plot(target_train, label='Target')
plt.legend(loc='best')
plt.show()

plt.plot(predict_test_mlp, label = 'Prev Test')
plt.plot(y_test, label='Target')
plt.legend(loc='best')
plt.show()

MSE(y_test, predict_test)
# Determinação do erro associado às previsões no conjunto de teste
print("Test MSE: %.10f" % mse(y_test, predict_test_mlp))
print("Test MAPE: %.10f" % mape(y_test, predict_test_mlp))

"""**MODELO MISTO**"""

modelo_ARIMA.fit(y_train)

modelo_rf

modelo_mlp

"""##Modelo Híbrido - Média Aritmética = (1/2)*modelo_ARIMA + (1/2)*modelo_MLP

"""

for valor1 in previsoes_test_ARIMA:
  valor1 = (1/2)*valor1

for valor2 in predict_test_mlp:
  valor2 = (1/2)*valor2


#(1/2)*previsoes_y_test_ARIMA + (1/2)*predict_test_MLP

previsoes_test_ARIMA = pd.DataFrame(previsoes_test_ARIMA)

previsoes_test_ARIMA.shape

predict_test_mlp = pd.DataFrame(predict_test_mlp)

predict_test_mlp.shape

# Combinando as previsões dos modelos ARIMA e MLP Regressor para o conjunto de testes
previsoes_y_test_hibrido = []
for i in range(0, len(predict_test_mlp)):
  valor3 = (1/2)*previsoes_test_ARIMA[0][i] + (1/2)*predict_test_mlp[0][i]
  previsoes_y_test_hibrido.append(valor3)

plt.plot(previsoes_y_test_hibrido, label = 'Modelo Híbrido: Previsões (Teste)')
plt.plot(y_test, label = 'Valores reais')
plt.legend(loc='best')
plt.show()

# Determinação do erro associado às previsões no conjunto de teste
print("Test MSE: %.10f" % mse(y_test, previsoes_y_test_hibrido))
print("Test MAPE: %.10f" % mape(y_test, previsoes_y_test_hibrido))